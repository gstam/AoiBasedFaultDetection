# -*- coding: utf-8 -*-
"""full_aoi_diagnosis_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Sn_pgTS9ZPUOPs1Ev-AtI8_hHtOVllIu
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os.path
from os import path

#df_a2c = pd.read_csv("ed_5000_sn_4_mc_-100.0_s_Phh_0.999_s_Pff_1.0_s_Phsug_1.0_s_Pfsug_0.999_n_Phh_0.99_n_Phsud_0.0.txt", delim_whitespace=True, header=None)
#df_dqn = pd.read_csv("dqn_ed_5000_sn_2_mc_-100.0_s_Phh_0.999_s_Pff_1.0_s_Phsug_1.0_s_Pfsug_0.999_n_Phh_0.99_n_Phsud_0.0(2).txt", delim_whitespace=True, header=None)

def get_moving_average(np_input_array, window_size = 10):
    moving_average = np.zeros(len(np_input_array) - window_size)
    moving_average[0] = (np.sum(np_input_array[0:window_size])/window_size)
    j = 0
    for i in range(window_size, len(np_input_array)-1, 1):
        moving_average[j+1] = moving_average[j] + (np_input_array[i] - np_input_array[i-window_size])/window_size
        j += 1
    return moving_average


def get_episodes_total_reward(data_file, episode_duration, number_of_episodes):
    np_episode_total_reward = np.zeros(number_of_episodes)
    episode_reward = np.zeros(episode_duration)

    data = np.loadtxt(data_file)

    for i in range(number_of_episodes):
        begin = i*episode_duration
        end = begin + episode_duration
    #    lst_mean_rewards_a2c.append(df_a2c[begin:end][2].sum())
        episode_reward = data[begin:end, 2]
        # print(episode_reward)
        np_episode_total_reward[i] = (np.sum(episode_reward))
        # print(np_episode_total_reward[i])

    return np_episode_total_reward

def get_episode_states_and_actions(data_file, episode_duration, number_of_episodes, requested_episode):
    episode_states = np.zeros(episode_duration)
    episode_actions = np.zeros(episode_duration)
    episode_rewards = np.zeros(episode_duration)

    data = np.loadtxt(data_file)
    begin = requested_episode*episode_duration
    end = begin + episode_duration

    episode_states = data[begin:end, 0]
    episode_actions = data[begin:end, 1]
    episode_rewards = data[begin:end, 2]

    return episode_states, episode_actions, episode_rewards

def trajectory_plot():
    number_of_episodes = 250
    requested_episode = 50
    episode_duration = 5000
    data_file = '/home/gstam/Projects/AoiBasedFaultDetection/Trajectory_data/trajectories_of_250_episodes.txt'
    episode_states, episode_actions, episode_rewards = get_episode_states_and_actions(data_file, episode_duration, number_of_episodes, requested_episode)
    print(np.sum(episode_rewards))
    x = np.arange(episode_duration)
    fig = plt.figure()
    plt.plot(x, episode_actions)
    plt.plot(x, episode_states) 
    plt.xlabel('Time slot') 
    plt.ylabel("State") #plt.title(')
    #plt.legend()
    plt.savefig('episode_trajectory.pdf')



def main():
    episode_number = 250
    training_sessions = 10
    data_frame = np.zeros((training_sessions, episode_number))

    base ='/home/gstam/Projects/AoiBasedFaultDetection/Permanent_faults/'
    rl_algorithm = ['a3c_basis_exp_', 'dqn_basis_exp_']
    number_of_drl_algorithms = 2
    mean_reward = np.zeros((number_of_drl_algorithms, episode_number))
    std_across_training_sessions = np.zeros((number_of_drl_algorithms, episode_number))
    
    alg_index = 0
    for alg in rl_algorithm:
        for i in range(1, training_sessions+1):
            experiment_folder = f'{alg}{i}'
            data_file = f'{base}{experiment_folder}/episodes_total_rewards.txt'
            # print(data_file)
            data = np.loadtxt(data_file)
            # print(data)
            data_frame[i-1,:] = data
        
        mean_reward[alg_index, :] = np.mean(data_frame, 0)
        std_across_training_sessions[alg_index, :] = np.std(data_frame, 0)
        alg_index += 1

    # Error


    # The x-axis represents the number of the episode.
    x = np.arange(episode_number)
    student_distribution_parameter = 2.262
    y_error = (student_distribution_parameter / np.sqrt(training_sessions))*std_across_training_sessions
    
    fig = plt.figure()
    #plt.plot(dqn_total_reward_per_episode, label='DQN')
    plt.plot(x, mean_reward[0,:], label='A2C') 
    plt.fill_between(x, mean_reward[0,:]-y_error[0,:], mean_reward[0,:]+y_error[0,:], alpha=0.2)
    plt.plot(x, mean_reward[1,:], label='DQN') 
    plt.fill_between(x, mean_reward[1,:]-y_error[0,:], mean_reward[1,:]+y_error[0,:], alpha=0.2)
    plt.xlabel('Episode') 
    plt.ylabel("Average episode reward") #plt.title(')
    plt.legend()
    plt.savefig('permanent_faults_average_reward_vs_episode.pdf')
  
  #  print(mean_reward)
  #  print(data_frame[0,:])
  #  print(data_frame.shape)

if __name__ == '__main__':
    # main()
    trajectory_plot()
# My name is george!

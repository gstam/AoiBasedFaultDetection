# -*- coding: utf-8 -*-
"""aoi_diagnosis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C-AgUTdaZTdxz_VPITC-Wq25iMYW1CBi
"""

#google = !if [ -d 'GDrive/' ]; then echo "1" ; else echo "0"; fi
#if (google[0] is '0' ):
#   from google.colab import drive
#   drive.mount('/content/gdrive')
#!if [ -d 'GDrive/' ]; then echo "Connection to Google drive successful" ; else echo "Error to connect to Google drive"; fi

# -*- coding: utf-8 -*-
"""aoi_diagnosis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C-AgUTdaZTdxz_VPITC-Wq25iMYW1CBi
"""

# google = !if [ -d 'GDrive/' ]; then echo "1" ; else echo "0"; fi
# if (google[0] is '0' ):
#    from google.colab import drive
#    drive.mount('/content/gdrive')
# !if [ -d 'GDrive/' ]; then echo "Connection to Google drive successful" ; else echo "Error to connect to Google drive"; fi

import time
import math
import collections

import numpy as np
from numpy.random import default_rng
import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F
from skimage.transform import resize
from IPython.display import clear_output, display
import torch.multiprocessing as mp
from IPython.display import clear_output
import os.path
from os import path
#from google.colab import files

Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])

class Sensor:
    def __init__(self, sensor_health_status, sensor_health_status_transition_matrix, p_status_update_generation):
        self.health_status = sensor_health_status
        self.health_status_transition_matrix = sensor_health_status_transition_matrix
        self.p_status_update_generation = p_status_update_generation
        self.queue_of_status_updates = []
        self.queue_capacity = 1
       
    def reset(self):
        self.health_status = HEALTHY_SENSOR
        self.queue_of_status_updates = []

    def get_health_status(self):
        return self.health_status


    def get_queue_of_status_updates(self):
        return self.queue_of_status_updates

    def apply_action(self, action):
        if self.health_status == FAULTY_SENSOR and action == REPAIR_SENSOR:
            self.health_status = HEALTHY_SENSOR
            

    def perform_sensor_state_transition(self):
        sample_number_for_sensor_health = np.random.default_rng().uniform(0.0, 1.0, 1)
        if self.health_status == HEALTHY_SENSOR:
            if sample_number_for_sensor_health <= self.health_status_transition_matrix[0][0]:
                pass
            else:
                self.health_status = FAULTY_SENSOR
        else:
            # if action == REPAIR_SENSOR: # or (action == 3): # both actions involve correcting the network's state
            #     self.health_status = HEALTHY_SENSOR
            # else:
            if sample_number_for_sensor_health <= self.health_status_transition_matrix[1][1]:
                pass
            else:
                self.health_status = HEALTHY_SENSOR


    def generate_status_update(self, current_time):
        sample_number_for_status_update_generation = np.random.default_rng().uniform(0.0, 1.0, 1)
        # print(f"Status update generator value: {sample_number_for_status_update_generation} while health status: {self.p_status_update_generation[self.health_status]}")
        if sample_number_for_status_update_generation <= self.p_status_update_generation[self.health_status]:
          if len(self.queue_of_status_updates) < self.queue_capacity:
            self.queue_of_status_updates.append(current_time)
          else:
            self.queue_of_status_updates[0] = (current_time)


    def deliver_status_update(self, network):
        packet_delivery = False
        packet_generation_time = -1
        sample_number_for_status_update_delivery = np.random.default_rng().uniform(0.0, 1.0, 1)
        if sample_number_for_status_update_delivery <= network.p_status_update_delivery[network.health_status]:
            if len(self.queue_of_status_updates) > 0:
                packet_delivery = True
                packet_generation_time = self.queue_of_status_updates.pop(0)
                #print("len(): {}, packet_delivery: {}, self.queue_of_status_updates.pop(0): {}".format(len(self.queue_of_status_updates), packet_delivery,self.generation_time_of_last_status_update_delivered))
        return packet_delivery, packet_generation_time


class Network: 
    def __init__(self, network_health_status, network_health_status_transition_matrix, p_status_update_delivery):
        self.health_status = network_health_status
        self.health_status_transition_matrix = network_health_status_transition_matrix
        self.p_status_update_delivery = p_status_update_delivery
        

    def reset(self):
        self.health_status = HEALTHY_NETWORK

    def get_health_status(self):
        return self.health_status

    def apply_action(self, action):
        if self.health_status == FAULTY_NETWORK and action == REPAIR_NETWORK:
            self.health_status = HEALTHY_NETWORK
        
    def perform_network_state_transition(self):
        sample_number_for_network_health = np.random.default_rng().uniform(0.0, 1.0, 1)
        if self.health_status == HEALTHY_NETWORK:
            if sample_number_for_network_health <= self.health_status_transition_matrix[0][0]:
                pass
            else:
                self.health_status = FAULTY_NETWORK
        else:
            if sample_number_for_network_health <= self.health_status_transition_matrix[1][1]:
                pass
            else:
                self.health_status = HEALTHY_NETWORK


class System:
    def __init__(self, sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, results_file_name, trajectory_file_name):
        self.number_of_actions = action_number
        self.sensor_number = sensor_number
        self.sensor = []
        for _ in range(sensor_number):
            self.sensor.append(Sensor(HEALTHY_SENSOR, sensor_health_status_transition_matrix, p_status_update_generation))
            self.sensor.append(Sensor(HEALTHY_SENSOR, sensor_health_status_transition_matrix, p_status_update_generation))
        self.results_file_name = results_file_name
        self.trajectory_file = trajectory_file_name
        self.network = Network(HEALTHY_NETWORK, network_health_status_transition_matrix, p_status_update_delivery)
        self.episode_duration = episode_duration
        self.time = 0
        self.true_health_status = np.zeros(self.episode_duration)
        self.agent_actions = np.zeros(self.episode_duration)
        self.environment_rewards = np.zeros(self.episode_duration)
        self.total_reward = 0.0
        self.packet_delivery = np.zeros(self.sensor_number)
        self.packet_generation_time = np.zeros(self.sensor_number)
        self.generation_time_of_last_status_update_delivered = np.zeros(self.sensor_number)
        self.aoi = np.zeros(self.sensor_number)
        self.observation = np.zeros(self.sensor_number)
        self.plot_id = 0
    
    def sample_action(self):
        action = np.random.default_rng().integers(self.number_of_actions)
#         r = np.random.default_rng().uniform(0.0, 1.0, 1)
#         action = 0
#         if r < 0.8:
#             action = 0
#         elif r < 0.9:
#             action = 1
#         else:
#             action = 2
        return action

    def determine_system_state(self):
        health_status = 0
        for i in range(self.sensor_number):
            # get_health_status is binary
            health_status += self.sensor[i].get_health_status() * math.pow(2,(i+1))
        health_status += self.network.get_health_status()

        # if self.sensor[1].get_health_status() == 0 and self.sensor[0].get_health_status() == 0 and self.network.get_health_status() == 0:
        #     assert(health_status == 0)
        # elif self.sensor[1].get_health_status() == 0 and self.sensor[0].get_health_status() == 0 and self.network.get_health_status() == 1:
        #     assert(health_status == 1)
        # elif self.sensor[1].get_health_status() == 0 and self.sensor[0].get_health_status() == 1 and self.network.get_health_status() == 0:
        #     assert(health_status == 2)
        # elif self.sensor[1].get_health_status() == 0 and self.sensor[0].get_health_status() == 1 and self.network.get_health_status() == 1:
        #     assert(health_status == 3)
        # elif self.sensor[1].get_health_status() == 1 and self.sensor[0].get_health_status() == 0 and self.network.get_health_status() == 0:
        #     assert(health_status == 4)
        # elif self.sensor[1].get_health_status() == 1 and self.sensor[0].get_health_status() == 0 and self.network.get_health_status() == 1:
        #     assert(health_status == 5)
        # elif self.sensor[1].get_health_status() == 1 and self.sensor[0].get_health_status() == 1 and self.network.get_health_status() == 0:
        #     assert(health_status == 5)
        # elif self.sensor[1].get_health_status() == 1 and self.sensor[0].get_health_status() == 1 and self.network.get_health_status() == 1:
        #     assert(health_status == 7)
        return health_status
  

    def step(self, action):
        #import pdb; pdb.set_trace()
        reward = 0.0
        maintenance_cost = 0.0
        
        is_done = False
        
        self.time +=1
        self.agent_actions[self.time-1] = action
        
        if self.time < self.episode_duration:
            # Determine the true health status of the system as an integer ranging from 0 to 7.
            self.true_health_status[self.time-1] = self.determine_system_state()

            # Repair sensors
            if action == REPAIR_SENSOR:
                maintenance_cost = MAINTENANCE_COST
                for i in range(self.sensor_number):
                    self.sensor[i].apply_action(action)
            
            # Repair network
            if action == REPAIR_NETWORK:
                maintenance_cost = MAINTENANCE_COST
                self.network.apply_action(action)

            # Deliver a status update (if one is availble) with some probability based on the health status of the network.
            for i in range(self.sensor_number):
                self.packet_delivery[i], self.packet_generation_time[i]  = self.sensor[i].deliver_status_update(self.network)
                if self.packet_delivery[i]: 
                    self.generation_time_of_last_status_update_delivered[i] = self.packet_generation_time[i]
                # generate a new status update with some probability that depends on the health status of the sensor.
                self.sensor[i].generate_status_update(self.time)
                # perform a state change for the sensor to the same or to a different state
                self.sensor[i].perform_sensor_state_transition()
                
            # Perform a state change for the network to the same or to a different state
            self.network.perform_network_state_transition()
            
            previous_observation = np.copy(self.aoi)
            
            # Calculate the resulting AoI for the system.
            for i in range(self.sensor_number):
                self.aoi[i] = self.time - self.generation_time_of_last_status_update_delivered[i]
                if self.aoi[i] < 0:
                    print(f'Negative AoI: {self.aoi[i]}')

            # AoI is the feature of the system observed by the agent.
            self.observation = np.round(self.aoi)
            if maintenance_cost == MAINTENANCE_COST:
                reward = 1/(maintenance_cost + np.mean(self.aoi))
            else: 
                reward = 1/np.mean(self.aoi)
            info = None
            
            self.total_reward += reward 
        
            # print(f"h: {self.true_health_status[self.time-1] } p_o: {previous_observation} a: {action}, o: {self.observation},r: {reward}")
        else:
            self.observation.fill(0.0)
            reward = 0.0
            is_done = True
            info = None
            if PLOT_RESULTS == True:
                self._plot_episode()
            if SAVE_RESULTS_TO_FILE:
                self._save_results_to_file()
            self.reset()

        self.environment_rewards[self.time-1] = reward
        return self.observation, reward, is_done, info

    def _save_results_to_file(self):
        results_file_handle = open(self.results_file_name, 'a+')
        results_file_handle.write(f'{self.total_reward}\n')
        results_file_handle.close()
        # Save a trajectory file that records states, actions and rewards.
        results_file_handle = open(self.trajectory_file, 'a+')
        np.savetxt(results_file_handle, np.hstack((self.true_health_status.reshape(self.episode_duration,1), self.agent_actions.reshape(self.episode_duration,1), self.environment_rewards.reshape(self.episode_duration,1))), fmt='%d %d %f', delimiter=',')
        results_file_handle.close()

    def _plot_episode(self):
        # if episode_count % 10 == 0:
        fig, ax = plt.subplots(2,1)
        plot_range = self.episode_duration
        x = np.arange(plot_range)
        ax[0].step(x, self.true_health_status[0:plot_range])    
        ax[1].step(x, self.agent_actions[0:plot_range])
        if LIVE_PLOTTING == False:  
            plt.savefig(str(self.plot_id))
            self.plot_id += 1
            plt.close(fig)
        else:
            plt.show()
            


    def reset(self):
        # print("Reseting the environment!")
        self.time = 0
        self.is_done = False
        self.total_reward = 0.0
        self.generation_time_of_last_status_update_delivered = np.zeros(self.sensor_number)

        #self.true_health_status = np.zeros(self.episode_duration)
        #self.agent_actions = np.zeros(self.episode_duration)
        for i in range(self.sensor_number):
            self.sensor[i].reset()
        self.network.reset()


class DQN(nn.Module):
    def __init__(self, observation_size, hidden_size, n_actions):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(observation_size, hidden_size),
            nn.ReLU(),
            # nn.Linear(hidden_size, hidden_size),
            #  nn.ReLU(),
            # nn.ReLU(),
            nn.Linear(hidden_size, n_actions)
        )

    def forward(self, x):
        #print(f"neural network input: {x}")    
        return self.net(x)


class ExperienceBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def print_experience_buffer(self, indices):
        print(f"The length of the buffer is: {len(self.buffer)}")
        for idx in indices:
           print(f"Buffer: {self.buffer[idx]}")

    def __len__(self):
        return len(self.buffer)

    def append(self, experience):
        #print(f"ExperienceBuffer before appending: {experience}")
        self.buffer.append(experience)
        #self.print_experience_buffer()

    def pop(self):
        return self.buffer.pop()

    def sample(self, batch_size):
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        # print(f"Buffer: {self.buffer[indices[0]]}")
        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])
        # print(f"States: {states[0]}, Actions: {actions[0]}, Dones: {dones[0]}, Next States: {next_states[0]}")
        # self.print_experience_buffer(indices)
        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_states)

    
        # for exp in len(self.buffer):
        #    #print(f"Buffer: {exp}")
        #    print(self.buffer.pop())


class InformationVector:
    def __init__(self, number_of_past_observations, number_of_past_actions):
        self.number_of_past_observations = number_of_past_observations
        self.number_of_past_actions = number_of_past_actions
        self.past_observations = np.zeros(number_of_past_observations)
        self.past_actions = np.zeros(number_of_past_actions)
        self.information_vector = np.zeros(number_of_past_observations + number_of_past_actions)

    def reset(self):
        self.information_vector = np.zeros(self.number_of_past_observations + self.number_of_past_actions)

    def _append_observation(self, observation):
        assert isinstance(observation, float), "Observation is not float" 
        # Roll matrix values one step to the left
        self.past_observations = np.roll(self.past_observations, 1)
        # Assign at the last position of the matrix the new observation
        self.past_observations[0] = observation
        return self.past_observations

    def _append_action(self, action):
        assert isinstance(action, float), "Observation is not float" 
        # Roll matrix values one step to the left
        self.past_actions = np.roll(self.past_actions, 1)
        # Assign at the last position of the matrix the new action
        self.past_actions[0] = action
        return self.past_actions
    
    def reset_information_vector(self):
        self.information_vector = np.zeros(self.number_of_past_observations + self.number_of_past_actions)

    def update_information_vector(self, observation, action):
        self._append_observation(observation)
        self._append_action(action)
        self.information_vector[0 : self.number_of_past_observations] = self.past_observations #np.hstack((self._append_observation(observation), self._append_action(action)))
        self.information_vector[self.number_of_past_observations : (self.number_of_past_observations + self.number_of_past_actions)] = self.past_actions
        return self.information_vector


class Agent:
    def __init__(self, env, observation_size, hidden_size, action_number): #, number_of_past_observations, number_of_past_actions
        self.env = env
        # self.number_of_past_observations = number_of_past_observations
        # self.number_of_past_actions = number_of_past_actions
        # self.I = InformationVector(number_of_past_observations, number_of_past_actions)
        # self.new_I = InformationVector(number_of_past_observations, number_of_past_actions)
        self.observation_size = observation_size
        self.state = np.zeros(self.observation_size, dtype = np.float32)
        self.next_state = np.zeros(self.observation_size, dtype = np.float32)
         # Neural network models.
        self.net = DQN(observation_size, hidden_size, action_number)
        self.tgt_net = DQN(observation_size, hidden_size, action_number)
        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)
        # Exprerience replay buffer
        self.exp_buffer = ExperienceBuffer(buffer_size)
        self._reset()
        
    def _reset(self):
        self.env.reset()
        self.state = np.zeros(self.observation_size)
        # self.I.reset() 
        # self.new_I.reset()
        self.total_reward = 0.0

    
    @torch.no_grad()
    def play_step(self, net, epsilon):

        done_reward = None
        # Select action with Îµ-greedy algorithm
        if np.random.random() < epsilon:
            # A modified epsilon-greedy algorithm that provides the agent with more experience 
            # for cases where the maintenance action is not taken for long periods of time.
            # if np.random.random() < 0.8:
            #     action = NO_MAINTENANCE
            # else:
            action = env.sample_action()
        else:
            #np_a_state = self.I.information_vector #np.array([self.information_vector])
            # print(f"np_a_state: {np_a_state}")
            torch_state = torch.FloatTensor(self.state) #self.I.information_vector)
            # print(f"torch_state: {torch_state}")
            state_action_values = net(torch_state)
            # print(f"y: {state_action_values}")
            _, index_of_best_action = torch.max(state_action_values, dim=0)#, dim=1)
            # print(f"indices: {index_of_best_action}")
            action = int(index_of_best_action.item()) #item() returns the value of the index of the best action
            # print(action)
        
        # Make a step in the environment and get back reinforcing feedback.
        new_observation, reward, is_done, info = self.env.step(action)
        #new_observation = new_observation/episode_duration
        self.total_reward += reward
        #self.new_I.update_information_vector(float(new_observation), float(action))
        #print(f"I: {self.I.information_vector}\t new I: {self.new_I.information_vector}")
        #print(self.I.information_vector.shape)
        #state = np.copy(self.I.information_vector)
        #next_state = np.copy(self.new_I.information_vector)
        self.next_state = np.copy(new_observation)

        exp = Experience(self.state, action, reward, is_done, self.next_state)
        #print(f"State: {exp.state} action: {exp.action} reward: {exp.reward}, done: {exp.done}, new_state: {exp.new_state}" )

        #test_exp = self.exp_buffer.pop()
        #print(f"Test exp: {test_exp}")
        self.exp_buffer.append(exp)
        #self.exp_buffer.print_experience_buffer()
        self.state = self.next_state
        #self.I.update_information_vector(float(new_observation), float(action))
            
        if is_done:
            done_reward = self.total_reward
            self._reset()
        
        return is_done, done_reward
        
    def train(self, epsilon_start, epsilon_final, epsilon_decay_last_stage, learning_duration, replay_start_size, batch_size, sync_target_network, net_file, file_name):    
        epsilon = epsilon_start
        episode_rewards_list = []
        best_mean_reward = 0
        mean_reward = None
        step_count = 0
        episode_count = 0
        
        for step_count in range(learning_duration):
            epsilon = max(epsilon_final, epsilon_start - step_count / epsilon_decay_last_stage)
                    
            # if episode_count % 10 == 0:
            #     episode_is_done, episode_reward = self.play_step(self.net, 0.0) #
            # else:
            episode_is_done, episode_reward = self.play_step(self.net, epsilon)
            
            # Under some improvement condition save the parameters of the netwrok.
            if episode_is_done: #step_count % 1000 == 0:
                episode_rewards_list.append(episode_reward)
                print(f'Episode: {episode_count} Total reward: {episode_reward}')
      
                if episode_count % 10 == 0:
                    torch.save(self.net.state_dict(), net_file)

                episode_count += 1


            if len(self.exp_buffer) < replay_start_size:
                continue
            
            if step_count % sync_target_network == 0:
                self.tgt_net.load_state_dict(self.net.state_dict())
                print("Target network update...")

            self.optimizer.zero_grad()
            batch = self.exp_buffer.sample(batch_size)
            loss_t = calc_loss(batch, self.net, self.tgt_net)
            loss_t.backward()
            self.optimizer.step()
       
        #files.download(file_name)
        return 0

    # The env is potentially different than the one the agent trained with
    # The agent will load a model file created during training.
    def play_episode(self, env, net_file):
	# This is my comment at github
        print("WARNING: The agent should fit the environment. This is not automatically tested!")
        model = self.net
        # state_dict = torch.load(net_file)
        model.load_state_dict(torch.load(net_file))
        for step_count in range(env.episode_duration):
            episode_is_done, episode_reward = agent.play_step(model, 0.0) #
            if episode_is_done:
                print(f"Episode Reward: {episode_reward}")
                # fig, ax = plt.subplots(2,1)
                # plot_range = episode_duration
                # x = np.arange(plot_range)
                # ax[0].step(x, self.env.true_health_status[0:plot_range])    
                # ax[1].step(x, self.env.agent_actions[0:plot_range])   
                # plt.show()
                results_file_handle = open('./Trajectory_data/trajectory_file', 'a+')
                np.savetxt(results_file_handle, np.hstack((self.env.true_health_status.reshape(env.episode_duration,1), self.env.agent_actions.reshape(self.env.episode_duration,1), self.env.environment_rewards.reshape(self.env.episode_duration,1))), fmt='%d %d %f', delimiter=',')
                results_file_handle.close()



    @torch.no_grad()
    def print_policy(self, env, net_file):
        aoi_range = 80

        policy = np.zeros((aoi_range, aoi_range))
        state = np.zeros(self.observation_size, dtype = np.float32)
        model = self.net
        state_dict = torch.load(net_file)
        model.load_state_dict(state_dict)
        for observation_1 in range(aoi_range):
            for observation_2 in range(aoi_range):
                state[0] = observation_1
                state[1] = observation_2
                #np_a_state = self.I.information_vector #np.array([self.information_vector])
                # print(f"np_a_state: {np_a_state}")
                torch_state = torch.FloatTensor(state) #self.I.information_vector)
                # print(f"torch_state: {torch_state}")
                state_action_values = model(torch_state)
                # print(f"y: {state_action_values}")
                _, index_of_best_action = torch.max(state_action_values, dim=0)#, dim=1)
                # print(f"indices: {index_of_best_action}")
                policy[observation_1, observation_2] = int(index_of_best_action.item()) #item() returns the value of the index of the best action
                print(np.where(policy == 2))
                # print(action)    
        #fig = plt.figure()
        #ax = Axes3D(fig)
        #x = np.arange(0, aoi_range, 1)
        #y = x
        #x, y = np.meshgrid(x, y)
        #ax.plot_wireframe(x, y, policy)
        plt.imshow(policy)
        plt.show()
        
 
def calc_loss(batch, net, tgt_net, device="cpu"):
    states, actions, rewards, dones, next_states = batch

    # states = states.reshape(batch_size, observation_size)
    # next_states = next_states.reshape(batch_size, observation_size)
    states_v = torch.FloatTensor(states).to(device)#FloatTensor(states).to(device) # np.array(states, copy=False)).to(device)
    #import pdb; pdb.set_trace()
    #print(next_states)
    next_states_v = torch.FloatTensor(next_states).to(device)#torch.FloatTensor(next_states).to(device) #np.array(next_states, copy=False)).to(device)
    actions_v = torch.tensor(actions).to(device)
    rewards_v = torch.FloatTensor(rewards).to(device)
    done_mask = torch.BoolTensor(dones).to(device)

    # gather() operates on a tensor and from a [batch_size, ACTION_SIZE] matrix it gathers values along dim=1, i.e., columns by using an index matrix [batch_size, action_selected] https://pytorch.org/docs/master/generated/torch.gather.html
    # unsqueeze() operates on a tensor adds a dimension of size one at the end of actions_v, i.e., actions_v has size [batch_size] and after unsqueeze it has size [batch_size, 1] https://pytorch.org/docs/stable/generated/torch.unsqueeze.html
    # squeeze() removes a dimension of size 1 from a tensor, i.e., actions_v is converted from a size [batch_size, 1] to a [batch_size] tensor. https://pytorch.org/docs/stable/generated/torch.squeeze.html
    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    #print(state_action_values)
    with torch.no_grad():
        next_state_values = tgt_net(next_states_v).max(1)[0]
        next_state_values[done_mask] = 0.0
        next_state_values = next_state_values.detach()

    expected_state_action_values = next_state_values * gamma + rewards_v
    #nn.MSEloss() creates a loss object that accepts as input two vectors https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html
    return nn.MSELoss()(state_action_values, expected_state_action_values)


class ActorCritic(nn.Module):
    def __init__(self, obseration_size):
        super(ActorCritic, self).__init__()
        self.l1 = nn.Linear(obseration_size,32)
        self.l2 = nn.Linear(32,128)
        self.actor_lin1 = nn.Linear(128,3)
        self.l3 = nn.Linear(128,32)
        self.critic_lin1 = nn.Linear(32,1)

    def forward(self,x):
        x = F.normalize(x,dim=0)
        y = F.relu(self.l1(x))
        y = F.relu(self.l2(y))
        actor = F.log_softmax(self.actor_lin1(y),dim=0)
        c = F.relu(self.l3(y.detach()))
        critic = torch.tanh(self.critic_lin1(c))
        return actor, critic   


def update_params(worker_opt, values, logprobs, rewards, G,  cost_to_go_horizon, clc=0.2, gamma=1):
    sample_length = len(rewards)
    rewards = torch.Tensor(rewards) #.flip(dims=(0,)).view(-1)
    logprobs = torch.stack(logprobs) #.flip(dims=(0,)).view(-1) #to Tensor and reverse
    values = torch.stack(values) #.flip(dims=(0,)).view(-1) #to Tensor and reverse
    Returns = []
    #G #torch.Tensor([0])#rewards_[0]
    #Ret.append(ret_)
    for h in range(rewards.shape[0]):
        if h < (rewards.shape[0] - cost_to_go_horizon):
            value = torch.sum(rewards[h:(h+cost_to_go_horizon)])
        else:
            value = torch.sum(rewards[h:])
        Returns.append(value)

    if len(Returns) != rewards.shape[0]:
        print(f'Wrong length for Returns')

    Returns = torch.stack(Returns).view(-1)
    Returns = F.normalize(Returns,dim=0)
    actor_loss = -1*logprobs * (Returns - values.detach())
    critic_loss = torch.pow(values - Returns,2)
    loss = actor_loss.sum() + clc*critic_loss.sum()
    loss.backward()

    worker_opt.step()
    return actor_loss, critic_loss, len(rewards)
    # rewards = torch.Tensor(rewards).flip(dims=(0,)).view(-1)
    # logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1) #to Tensor and reverse
    # values = torch.stack(values).flip(dims=(0,)).view(-1) #to Tensor and reverse
    # Returns = []
    # ret_ = G
    # for r in range(rewards.shape[0]):
    #     ret_ = rewards[r] + gamma * ret_
    #     Returns.append(ret_)
    # Returns = torch.stack(Returns).view(-1)
    # Returns = F.normalize(Returns,dim=0)
    # actor_loss = -1*logprobs * (Returns - values.detach())
    # critic_loss = torch.pow(values - Returns,2)
    # loss = actor_loss.sum() + clc*critic_loss.sum()
    # loss.backward()
    # worker_opt.step()
    # return actor_loss, critic_loss, len(rewards)
    

def run_episode(worker_env, worker_model, N_steps):
    state = torch.from_numpy(worker_env.observation).float()
    values, logprobs, rewards = [], [], []
    done = False
    j=0
    G = torch.Tensor([0])
    while (j < N_steps and done == False):
        j+=1
        #run actor critic model
        policy, value = worker_model(state)
        values.append(value)
        #sample action
        logits = policy.view(-1)
        action_dist = torch.distributions.Categorical(logits=logits)
        action = action_dist.sample()
        logprob_ = policy.view(-1)[action]
        logprobs.append(logprob_)
        state_, reward, done, info = worker_env.step(action.detach().numpy())
        state = torch.from_numpy(state_).float()
        # if done:
        #     reward = -10
        #     worker_env.reset()
        # else:
        #     reward = 1.0
        if done: 
            pass
        else:
            G = value.detach()
        rewards.append(reward)
    
    # print([v.data for v in values])
    # print([l.data for l in logprobs])
    # print(rewards)
        # print(logprobs.data)
        # print(rewards.data)
    return values, logprobs, rewards, G


def worker(t, worker_model, counter, params, losses, sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, file_name, model_path, trajectory_path): #q is mp Queue
    print("In process {}".format(t,))
    # start_time = time.time()
    #play n steps of the game, store rewards
    worker_env = System(sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, file_name, trajectory_path)#gym.make("CartPole-v1")
    #worker_env.reset()
    worker_opt = optim.Adam(lr=1e-4,params=worker_model.parameters())
    worker_opt.zero_grad()
    cost_to_go_horizon = 10
    step_number = 100*cost_to_go_horizon
    
    episode_rewards_list = []

    for i in range(params['epochs']):
        episode_reward = 0
        for j in range(int(episode_duration/step_number)):
            worker_opt.zero_grad()
            #stores
            values, logprobs, rewards, G = run_episode(worker_env,worker_model, step_number)
            # [print(v.data.data.cpu.value for v in values]
            actor_loss, critic_loss, eplen = update_params(worker_opt, values, logprobs, rewards, G, cost_to_go_horizon)
            # counter.value = counter.value + 1
            # losses.put(eplen)
            episode_reward += sum(rewards)
            # print("Process: {} Maxrun: {} ALoss: {} CLoss: {} Total reward: {}".format(t, eplen, actor_loss.detach().mean().numpy(), critic_loss.detach().mean().numpy(), sum(rewards)))

        if SAVE_MODEL == True:
            torch.save(worker_model, model_path)
        #files.download(file_name)
        print(f"Episode: {i} Episode reward: {episode_reward}")
       

if __name__ == "__main__":
    training_sessions = 20 
    training_sessions_start = 1
    training_sessions_stop = training_sessions_start + training_sessions
    run_dqn_experiment = True 
    run_a3c_experiment = False 
    episode_number = 150 
    episode_duration = 5000
    descriptive_name = 'vanilla_epsilon_greedy'
    path_name = './Data/Intermittent_sensor_and_network_faults/' 
    # Logging
    PLOT_RESULTS = False #True
    LIVE_PLOTTING = False #True
    SAVE_MODEL = True
    SAVE_RESULTS_TO_FILE = True

    # Actions
    NO_MAINTENANCE = 0
    REPAIR_NETWORK = 1
    REPAIR_SENSOR = 2

    # Reward
    MAINTENANCE_COST = 100.0

    # State description
    HEALTHY_SENSOR = 0
    HEALTHY_NETWORK = 0
    FAULTY_SENSOR = 1
    FAULTY_NETWORK = 1

    # Q-learning
    gamma = 1.0 #0.99
    batch_size = 32
    buffer_size = 500000
    learning_rate = 1e-4
    sync_target_network = 20000
    replay_start_size = 50000
    
    learning_duration = episode_number*episode_duration
    control_step_duration = 1
    epsilon_decay_last_stage = 500000
    epsilon_start = 1.0
    epsilon_final = 0.01

    # Neural network parameters.
    # number_of_past_observations = 5
    # number_of_past_actions = number_of_past_observations - 1

    # Currently we have one AoI value for each sensor.
    observation_size = 4 # #number_of_past_observations + number_of_past_actions
    hidden_size = 128

    # Simulation parameters
    # sensor probability to make a transition from healthy to healthy state
    s_Phh = 0.999
    # sensor probability to make a transition from faulty to faulty state
    s_Pff = 0.9
    # sensor probability to generate a status update when healthy
    s_Phsug = 1.0
    # sensor probability to generate a status update when faulty
    s_Pfsug = 0.0 
    # network transition probability from healthy to healthy state
    n_Phh = 0.999
    # network transition probability from faulty to faulty state
    n_Pff = 0.9 
    # network status update delivery probability when in healthy state
    n_Phsud = 0.99
    # network status update delivery when in faulty stae
    n_Pfsud = 0.0
    sensor_health_status_transition_matrix = np.array([[s_Phh, 1.0 - s_Phh], 
                                                       [1.0-s_Pff, s_Pff]])
    
    
    network_health_status_transition_matrix = np.array([[n_Phh, 1.0 - n_Phh], 
                                                        [1.0 - n_Pff, n_Pff]])

    # First value corresponds to healthy probability of a status update generation/arrival while the second value corresponds to faulty status update generation/arrival probability.
    p_status_update_generation = np.array([s_Phsug, s_Pfsug])
    p_status_update_delivery = np.array([n_Phsud, n_Pfsud])

    # Initially all system components are healthy.
    sensor_health_status = 0
    network_health_status = 0
    
    # Simulation parameters
    is_done = False
    sensor_number = observation_size    
    action_number = 3
    
    if run_dqn_experiment:
    	for t in range(training_sessions_start, training_sessions_stop):
            folder_name = f'dqn_{descriptive_name}_exp_{t}/'
            #f'dqn_{descriptive_name}_sess_{t}_ed_{episode_duration}_sn_{observation_size}_mc_{MAINTENANCE_COST}_s_Phh_{s_Phh}_s_Pff_{s_Pff}_s_Phsug_{s_Phsug}_s_Pfsug_{n_Phh}_n_Phh_{n_Phsud}_n_Phsud_{n_Pfsud}/'
            if not os.path.exists(f'{path_name}{folder_name}'):
                os.makedirs(f'{path_name}{folder_name}')
            file_name = 'episodes_total_rewards.txt' 
            net_file = 'dqn_trained_model.pth'
            trajectory_file_name = 'modified_epsilon_greedy_trajetory.txt'

            results_path = f'{path_name}{folder_name}{file_name}'
            model_path = f'{path_name}{folder_name}{net_file}'
            trajectory_path = f'{path_name}{trajectory_file_name}'
            env = System(sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, results_path, trajectory_path)
            agent = Agent(env, observation_size, hidden_size, action_number) #, number_of_past_observations, number_of_past_actions
            trained_net = agent.train(epsilon_start, epsilon_final, epsilon_decay_last_stage, learning_duration, replay_start_size, batch_size, sync_target_network, model_path, path)
            # agent.play_episode(env, './dqn_trained_model.pth')
	        #agent.play_episode(env, net_file)
	        # agent.print_policy(env, net_file)
	        #files.download(file_name) 
    if run_a3c_experiment:
        for t in range(training_sessions_start, training_sessions_stop):
            folder_name = f'a3c_{descriptive_name}_exp_{t}/'
            #f'a3c_{descriptive_name}_sess_{t}_ed_{episode_duration}_sn_{observation_size}_mc_{MAINTENANCE_COST}_s_Phh_{s_Phh}_s_Pff_{s_Pff}_s_Phsug_{s_Phsug}_s_Pfsug_{n_Phh}_n_Phh_{n_Phsud}_n_Phsud_{n_Pfsud}/'
            if not os.path.exists(f'{path_name}{folder_name}'):
                os.makedirs(f'{path_name}{folder_name}')
            file_name = 'episodes_total_rewards.txt'
            model_file_name = f'a3c_trained_model.pth'
            trajectory_file_name = 'modified_epsilon_greedy_trajetory.txt'
            results_path = f'{path_name}{folder_name}{file_name}'
            model_path = f'{path_name}{folder_name}{model_file_name}'
            trajectory_path = f'{path_name}{trajectory_file_name}'
            if path.exists(model_path):
                print("Loading existing model")
                MasterNode = torch.load(model_path)
                MasterNode.eval()
            else:
                print("Creating a new a3c model")
            MasterNode = ActorCritic(observation_size)
            # def worker(t, worker_model, counter, params, losses, sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, file_name)
            # MasterNode.share_memory()
            processes = []
            params = {
	            'epochs': episode_number,
	            'n_workers':1,
	        }
            i = 0 
            counter = 0
            losses = 0
            worker(t, MasterNode, counter, params, losses, sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, results_path, model_path, trajectory_path)
            # counter = mp.Value('i',0)
            # losses = mp.Queue()
            # for i in range(params['n_workers']):
            #     p = mp.Process(target=worker, args=(i,MasterNode,counter,params,losses,sensor_health_status_transition_matrix, network_health_status_transition_matrix, p_status_update_generation, p_status_update_delivery, episode_duration, sensor_number, action_number, file_name))
            #     p.start()
            #     processes.append(p)
            # for p in processes:
            #     p.join()
            # for p in processes:
            #     p.terminate()

            torch.save(MasterNode, model_path)
	        # https://pytorch.org/tutorials/beginner/saving_loading_models.html
            # print(counter.value,processes[0].exitcode)
            #files.download(model_file_name)
	        #files.download(file_name)
